{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.2 文本分类\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import csv\n",
    "def preprocessing(text):\n",
    "    #text = text.decode(\"utf8\")\n",
    "    tokens=[word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]#sent cut sent and word cut word\n",
    "    stop=stopwords.words('english')\n",
    "    tokens=[token for token in tokens if token not in stop]\n",
    "    tokens=[word for word in tokens if len(word)>=3]\n",
    "    tokens=[word.lower() for word in tokens]\n",
    "    #\n",
    "    lmtzr=WordNetLemmatizer()\n",
    "    tokens=[lmtzr.lemmatize(word) for word in tokens]\n",
    "    preprocessed_text=''.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "sms = open('SMSSpamCollection','rb')#use byte stream to read the file\n",
    "sms_data=[]\n",
    "sms_labels=[]\n",
    "csv_reader = csv.reader(codecs.iterdecode(sms, 'utf-8'),delimiter='\\t')#csv    use codecs to decode the byte to utf-8.\n",
    "#if not, we will face the problem such as \"gbk?\"\"text or string\"\"byte or string\"\n",
    "for line in csv_reader:\n",
    "    sms_labels.append(line[0])\n",
    "    sms_data.append(preprocessing(line[0]))\n",
    "sms.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set size for this classifier is 3900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#取样\n",
    "trainset_size = int(round(len(sms_data)*0.70))\n",
    "print('The training set size for this classifier is '+str(trainset_size)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ..., 'spam' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "x_train=np.array([''.join(el)for el in sms_data[0:trainset_size]])\n",
    "y_train=np.array([el for el in sms_labels[0:trainset_size]])\n",
    "x_test=np.array([''.join(el)for el in sms_data[trainset_size+1:]])\n",
    "y_test=np.array([el for el in sms_labels[trainset_size+1:len(sms_labels)]])\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ..., 'spam' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#something wrong with 6.3-2\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#sms_exp=[]\n",
    "#for line in sms_data:\n",
    "#    sms_exp.append(line[0])\n",
    "#vectorizer=CountVectorizer(min_df=1)\n",
    "#x_exp = vectorizer.fit_transform(sms_exp)\n",
    "#print(x_exp.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(min_df=2,ngram_range=(1,2),stop_words='english',strip_accents='unicode',norm='l2')\n",
    "X_train=vectorizer.fit_transform(x_train)\n",
    "X_test=vectorizer.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.3 bayes\n",
    "#but some wrong here so this chapter had been wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
