{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[S -> NP VP,\n",
       " VP -> V NP,\n",
       " V -> 'eats',\n",
       " V -> 'drinks',\n",
       " NP -> Det N,\n",
       " Det -> 'a',\n",
       " Det -> 'the',\n",
       " Det -> 'an',\n",
       " N -> 'president',\n",
       " N -> 'Obama',\n",
       " N -> 'apple',\n",
       " N -> 'coke']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.3\n",
    "import nltk\n",
    "from nltk import CFG\n",
    "toy_grammar = nltk.CFG.fromstring(\n",
    "'''\n",
    "S -> NP VP\n",
    "VP -> V NP\n",
    "V -> \"eats\" | \"drinks\"\n",
    "NP -> Det N\n",
    "Det -> \"a\" | \"the\" |\"an\"\n",
    "N -> \"president\" |\"Obama\"|\"apple\"|\"coke\"\n",
    "''')\n",
    "toy_grammar.productions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Mr.Obama/NNP\n",
      "  (VP\n",
      "    (V played/VBD)\n",
      "    (NP a/DT big/JJ role/NN)\n",
      "    (PP (P in/IN) (NP the/DT)))\n",
      "  Health/NNP\n",
      "  (NP insurance/NN bill/NN))\n"
     ]
    }
   ],
   "source": [
    "#4.4  使用正则表达式来构造解析树分析句子语法\n",
    "chunk_rules=(\"<.*>\",\"chunk everything\")\n",
    "import nltk\n",
    "from nltk.chunk.regexp import * \n",
    "reg_parser = RegexpParser(\n",
    "'''\n",
    "NP: {<DT>? <JJ>* <NN>*}\n",
    "P:{<IN>}\n",
    "V:{<V.*>}\n",
    "PP:{<P> <NP>}\n",
    "VP:{<V> <NP|PP>*}\n",
    "''')\n",
    "test_sent = \"Mr.Obama played a big role in the Health insurance bill\"\n",
    "test_sent_pos = nltk.pos_tag(nltk.word_tokenize(test_sent))\n",
    "paresed_out = reg_parser.parse(test_sent_pos)\n",
    "print (paresed_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5  dependency parsing   something wrong here\n",
    "#from nltk.parse.stanford import StanfordParser\n",
    "#english_parser = StanfordParser('stanford-parser.jar','stanfordparser-3.4-models.jar')\n",
    "#english_parser.raw_parse_sents((\"this is the english parser test\") Parse(ROOT(S(NP (DT this))(VP (VBZ is)(NP (DT the)(JJ english)(NN parser)(NN test))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  prime/JJ\n",
      "  minister/NN\n",
      "  (VP announced/VBD he/PRP)\n",
      "  (VP had/VBD asked/VBN)\n",
      "  the/DT\n",
      "  chief/JJ\n",
      "  government/NN\n",
      "  whip/NN\n",
      "  ,/,\n",
      "  Philip/NNP\n",
      "  Ruddock/NNP\n",
      "  ,/,\n",
      "  to/TO\n",
      "  (VP call/VB)\n",
      "  aspecial/JJ\n",
      "  party/NN\n",
      "  room/NN\n",
      "  meeting/NN\n",
      "  for/IN\n",
      "  9am/CD\n",
      "  on/IN\n",
      "  Monday/NNP\n",
      "  to/TO\n",
      "  (VP consider/VB)\n",
      "  the/DT\n",
      "  spill/NN\n",
      "  motion/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk.regexp import *\n",
    "test_sent=\"The prime minister announced he had asked the chief government whip, Philip Ruddock, to call aspecial party room meeting for 9am on Monday to consider the spill motion\"\n",
    "test_sent_pos = nltk.pos_tag(nltk.word_tokenize(test_sent))\n",
    "rule_vp = ChunkRule(r'(<VB.*>)?(<VB.*>)+(<PRP>)?','Chunk VPs')\n",
    "parser_vp = RegexpChunkParser([rule_vp],chunk_label='VP')\n",
    "print(parser_vp.parse(test_sent_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT prime/JJ minister/NN)\n",
      "  announced/VBD\n",
      "  he/PRP\n",
      "  had/VBD\n",
      "  asked/VBN\n",
      "  (NP the/DT chief/JJ government/NN whip/NN)\n",
      "  ,/,\n",
      "  (NP Philip/NNP Ruddock/NNP)\n",
      "  ,/,\n",
      "  to/TO\n",
      "  call/VB\n",
      "  (NP aspecial/JJ party/NN room/NN meeting/NN)\n",
      "  for/IN\n",
      "  9am/CD\n",
      "  on/IN\n",
      "  (NP Monday/NNP)\n",
      "  to/TO\n",
      "  consider/VB\n",
      "  (NP the/DT spill/NN motion/NN))\n"
     ]
    }
   ],
   "source": [
    "rule_np = ChunkRule(r'(<DT>?<RB>?)?<JJ|CD>*(<JJ|CD><,>)*(<NN.*>)+','Chunk NPs')\n",
    "parser_np = RegexpChunkParser([rule_np],chunk_label = 'NP')\n",
    "print(parser_np.parse(test_sent_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  prime/JJ\n",
      "  minister/NN\n",
      "  announced/VBD\n",
      "  he/PRP\n",
      "  had/VBD\n",
      "  asked/VBN\n",
      "  the/DT\n",
      "  chief/JJ\n",
      "  government/NN\n",
      "  whip/NN\n",
      "  ,/,\n",
      "  (PERSON Philip/NNP Ruddock/NNP)\n",
      "  ,/,\n",
      "  to/TO\n",
      "  call/VB\n",
      "  aspecial/JJ\n",
      "  party/NN\n",
      "  room/NN\n",
      "  meeting/NN\n",
      "  for/IN\n",
      "  9am/CD\n",
      "  on/IN\n",
      "  Monday/NNP\n",
      "  to/TO\n",
      "  consider/VB\n",
      "  the/DT\n",
      "  spill/NN\n",
      "  motion/NN)\n"
     ]
    }
   ],
   "source": [
    "#4.7 NER\n",
    "text = test_sent\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "for sent in tagged_sentences:\n",
    "    print (nltk.ne_chunk(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "IN = re.compile(r'.*\\bin\\b(?!b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG','LOC',doc,corpus='ieer',pattern=IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package ieer to\n",
      "[nltk_data]     C:\\Users\\20781\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\ieer.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
